{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8256a95c-72ed-4a6a-9d77-dfbfbab3497a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Quality check of JEDI dimensions\n",
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3d9d730-fab0-4615-a1c4-9c8a4fb3872e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Errors can occur at every step of a process, which also applies to the ingestion process. Therefore, it is extremely important to check after each ingestion whether it was performed correctly. \n",
    "The implementation of data validation for ingestion involves a series of steps to ensure the quality and integrity of the ingested datasets. Here is an overview of the key aspects of the implementation:\n",
    "\n",
    "1. Feature List Generation: The first step is to define a comprehensive list of features that need to be validated for each ingested dataset. This list encompasses various aspects such as data completeness, correctness, consistency, and conformity to predefined standards. It serves as a reference for the subsequent validation process.\n",
    "\n",
    "2. Descriptive Statistics Calculation: Once the dataset is ingested, descriptive statistics are computed for each feature. These statistics capture important characteristics of the data, including measures such as mean, standard deviation, minimum, maximum, and other relevant statistical indicators. These statistics provide valuable insights into the distribution and properties of the data.\n",
    "\n",
    "3. Anomaly detection: The computed descriptive statistics are then compared to the statistics derived from previously ingested datasets. A detection process is applied to identify any significant deviations or anomalies in the newly ingested data. This process uses the reference statistics of acceptable data as a baseline for detecting inconsistencies or unexpected patterns.\n",
    "\n",
    "4. Error Labelling and Data Incorporation: Based on the results of above, the ingested data is labelled either as acceptable or erroneous. If significant discrepancies are detected, the data is flagged as erroneous, indicating the presence of potential data quality issues. However, if no discrepancies are found, the data is labelled as acceptable, and its descriptive statistics are incorporated into the ensemble of reference statistics for future comparisons.\n",
    "\n",
    "5. Spatial Validation: In addition to the feature-based validation, spatial validation is performed to ensure the spatial integrity of the ingested data. This involves verifying that the spatial attributes have been preserved accurately during the ingestion process. It also includes checks for prop\n",
    "\n",
    "er re-projection, if applicable, to ensure the spatial relationships and alignments are maintained correctly.\n",
    "\n",
    "6. Reporting and Logging: Throughout the validation process, comprehensive reports and logs are generated to document the validation results. These reports provide detailed information about the validation outcomes, including any detected errors or anomalies. This documentation facilitates further analysis, investigation, and troubleshooting of data quality issues. This information should be part of the meta-data catalogue of each dataset.\n",
    "\n",
    "Methodology: ------------------\n",
    "\n",
    "The proposed method consists of two steps: first, a list of features to be checked after ingesting the dataset; secondly, an automatic anomaly detection method to identify deviations from previously ingested data and by means of feature vector validation This approach does not require domain experts to define data quality constraints or provide valid examples.\n",
    "\n",
    "The proposed method for data ingestion validation and anomaly detection consists of three key steps:\n",
    "\n",
    "· Step 1: List of features is generated, specifying the aspects that need to be examined after ingesting the dataset.\n",
    "\n",
    "· Step 2: Anomaly detection method to identify any deviations or anomalies in the newly ingested data.\n",
    "\n",
    "By implementing this method, data quality assessment becomes more efficient and less reliant on manual intervention. The automated nature of the approach allows for consistent and reliable anomaly detection, even without extensive domain expertise or predefined quality constraints.\n",
    "\n",
    "\n",
    "\n",
    "|     Feature                               |     Description                                                                                                                                                                                  |   |   |   |\n",
    "|-------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---|---|---|\n",
    "|     Duplicates                            |     Check for duplicate   entries                                                                                                                                                                |   |   |   |\n",
    "|     Date overlap                          |     Check for overlaps   in the date column, i.e., repeated dates with different values (only for   timeseries)                                                                                  |   |   |   |\n",
    "|     Date gaps                             |     Check for missing   dates between start and end date (only for timeseries)                                                                                                                   |   |   |   |\n",
    "|     No data values                        |     Verification of the   correct use of no data                                                                                                                                                 |   |   |   |\n",
    "|     Value types                           |     Check if data types   are correct (string, integer, float, datetime format)                                                                                                                  |   |   |   |\n",
    "|     Value encoding                        |     Check if the encoding of the data is correct (e.g.,   character encoding is utf-8; point (.) is used as decimal separator)     Check the special   characters in the different languages.    |   |   |   |\n",
    "|     Completeness                          |     The ratio of   not-NULL values                                                                                                                                                               |   |   |   |\n",
    "|     Count of distinctive   values         |     Number of distinct   values in the dataset                                                                                                                                                   |   |   |   |\n",
    "|     Ratio of the most   frequent value    |     Number of   occurrences for the most frequently repeated value, normalized by the batch   size                                                                                               |   |   |   |\n",
    "|     Maximum                               |     Maximum value of the   dataset                                                                                                                                                               |   |   |   |\n",
    "|     Mean                                  |     Mean value of the   dataset                                                                                                                                                                  |   |   |   |\n",
    "|     Minimum                               |     Minimum value of the   dataset                                                                                                                                                               |   |   |   |\n",
    "|     Standard deviation                    |     Standard deviation   of the dataset                                                                                                                                                          |   |   |   |\n",
    "|     Number of records                     |     Number of rows in   the dataset (e.g., number of polygons)                                                                                                                                   |   |   |   |\n",
    "|     Date range                            |     Start and end date   (only for timeseries)                                                                                                                                                   |   |   |   |\n",
    "|     Grid boundaries                       |     Top-left and bottom-right coordinates (only for   gridded datasets)     (GridNum or X,Y   values of the dimension)                                                                           |   |   |   |\n",
    "|     Data completeness                     |      Check to verify that the data set is   complete (total area, total number of features or pixel)                                                                                             |   |   |   |\n",
    "|                                           |                                                                                                                                                                                                  |   |   |   |\n",
    "|     Pixel size                            |     Check to verify that   the pixel size is correct – bases on AreaHa value                                                                                                                     |   |   |   |\n",
    "|     Number of attributes                  |     Verification that   all attributes of the table or vector dataset have been transmitted   correctly. – compare the metadata on JEDI with the dimension on Azure                              |   |   |   |\n",
    "|                                           |                                                                                                                                                                                                  |   |   |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a17ba5a-6992-41c4-a76e-36226d71f538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## (1) Example: Checking Census grid 2021 dimension "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ca5374e-625a-498a-b339-f9eb9f6336e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Dimension INFO:\n",
    "https://jedi.discomap.eea.europa.eu/Dimension/show?dimId=1967&fileId=992\n",
    "\n",
    "ESMS Census Grid 2021 - Population grid 1km²\n",
    "\n",
    "cwsblobstorage01/cwsblob01/Dimensions/D_Pop_census_2021_992_2023510_1km\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e18e28a-4b6e-4c6b-8406-df6f4c22c96a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# importing required python modules\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6db1f10-3d8a-4bb5-9c43-e91731707d05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " ###(1.2) Reading the dimension\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a46ec1f-05e3-4df4-a3d2-0ffbf581eda2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " ####(1.2.1) Reading the dimension and converting to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f61ede-808d-4609-8254-c22356dbbe1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reading the dimension and saving to a spark dataframe; just replace dimension folder name below to read from a different dimension\n",
    "dim_folder = \"D_Pop_census_2021_992_2023510_1km\"\n",
    "df_spark = spark.read.format(\"delta\").load(f\"dbfs:/mnt/trainingDatabricks/Dimensions/{dim_folder}/\")\n",
    "# converting the spark dataframe to a pandas dataframe\n",
    "df = df_spark.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "265cbce7-a65e-4f4a-b4ef-b02b3dd52874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " ####(1.2.2) Reading the dimension and converting to scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab22f4d9-f7cb-4e7d-8092-6fb617a4c565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "//##########################################################################################################################################\n",
    "//   THIS BOX reads all Dimensions (DIM) and Lookuptables (LUT) \n",
    "\n",
    "////  !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "spark.conf.set(\"spark.databricks.delta.formatCheck.enabled\",false)\n",
    "import spark.sqlContext.implicits._ \n",
    "//##########################################################################################################################################\n",
    "\n",
    "//// (0) ESMS Census Grid 2021 - Population grid 1km² ################################################################################\n",
    "// Reading the admin DIM:---------------------------------------------\n",
    "//https://jedi.discomap.eea.europa.eu/Dimension/show?dimId=1967&fileId=992\n",
    "//cwsblobstorage01/cwsblob01/Dimensions/D_Pop_census_2021_992_2023510_1km\n",
    "val parquetFileDF_Population  = spark.read.format(\"delta\").load(\"dbfs:/mnt/trainingDatabricks/Dimensions/D_Pop_census_2021_992_2023510_1km/\")             /// use load\n",
    "parquetFileDF_Population.createOrReplaceTempView(\"Population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bfc290d-ebea-495c-88f4-bf8591759759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###(1.3)  Overview of dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff3fd140-acf1-4aeb-ad10-2393f389566d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Current dimension has {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "print(\"These are the column names:\")\n",
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "703d361f-7a35-403c-8bab-69f7637d2b2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--- this function described the selected dimension table:\n",
    "DESCRIBE TABLE Population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5f5eb45-8f24-45cf-b285-2f7d200c74ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " ###(1.4) Explore the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cee80c0e-cd21-4a6b-a7c6-0a8a07148ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5 sample rows\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "868a0213-8eca-4133-8bbd-a1bd7e7e28f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 10 top rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce0c528d-18ab-4b88-b230-c5fc25e053ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " ###(1.5) Overall statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aa22731-8eaa-4713-a293-d986d189c14a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x) # setting the format for the output\n",
    "df.describe(percentiles=[.1, .25, .5, .75, .9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e13eebd1-059e-4e3f-ad87-80f1b4f1c8ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#checking for NaN, replace field to check name to check other fields\n",
    "field_to_check = 'ESTAT_OBSVALUET_2021_V10'\n",
    "df[field_to_check].isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b104d34-0491-4e72-8976-d5f57cb2d98e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " ###(1.6) Histogram of dimension figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12882228-c5e8-4f97-aedd-c1f0ed9979cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['ESTAT_OBSVALUET_2021_V10'] = df['ESTAT_OBSVALUET_2021_V10'].astype(float)\n",
    "pop_figs = df['ESTAT_OBSVALUET_2021_V10'].loc[df['ESTAT_OBSVALUET_2021_V10'] > 0]\n",
    "histo = px.histogram(pop_figs,\n",
    "                     x=\"ESTAT_OBSVALUET_2021_V10\",\n",
    "                     nbins=1000,\n",
    "                     log_y=True)\n",
    "histo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f3af169-d19a-41e7-adb6-4cde05097441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " ###(1.7)  Verifying figures for a known area (Luxembourg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "779b6095-2a88-4540-80d0-dde3360b1884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's check if population figures for Luxembourg make sense\n",
    "# read table with grid cells for Luxembourg\n",
    "df_spark = spark.read.format(\"delta\").load(f\"dbfs:/user/hive/warehouse/databricks_1_km_and_10_k_grids_for_lu/\")\n",
    "# converting the spark dataframe to a pandas dataframe\n",
    "df_lux = df_spark.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9122d687-46ed-4723-ac25-f74ec53f7df6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# quick check if import was OK\n",
    "df_lux.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49088935-df31-4a04-b437-416722976dc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# join dimension info to the lux dataframe\n",
    "#\n",
    "df.rename(columns = {'gridnum':'GridNum'}, inplace = True)\n",
    "merged_lux = pd.merge(df_lux, df, on='GridNum')\n",
    "merged_lux.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8253df4-0790-45f3-b46d-62c386f77880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# calculation of total population from the census grid\n",
    "total_pop_lux = int(merged_lux['ESTAT_OBSVALUET_2021_V10'].sum())\n",
    "print(f'The total population of Luxembourg from Eurostat Census Grid 2021 data is {total_pop_lux}\\nThe official figure from the World Bank and same year was 640064')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "D60_QAQC of data upload dimensions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}